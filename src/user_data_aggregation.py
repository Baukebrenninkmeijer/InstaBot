
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/2_user_data_aggregation.ipynb

import pandas as pd
import os
import datetime
import random
import logging
from InstagramAPI import InstagramAPI
from tqdm import tqdm
from time import sleep
import pickle
from scriptine import run
import sys
sys.path.append('src')
import lib


logger = logging.getLogger(__name__)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(name)-s %(levelname)-s : %(message)s')
fh = logging.FileHandler('logs/data_aggregation_users.log', encoding='utf-8')
fh.setLevel(logging.DEBUG)
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.INFO)
logger.addHandler(fh)

io_method = lib.DatabaseIO() #CsvIO
artefacts_path = 'artefacts'


def read_users():
    return io_method.read_data('users')


def write_users(df):
    logger.info('Saving users')
    io_method.write_data(df, 'users')
    logger.info('Done.')


def update_followers(users=None, skip=None, nr_new_following=200):
    logger.info(f'{"#"*10} Update followers {"#"*10}')
    start_time = datetime.datetime.now()
    logger.info(f'Start time: {start_time}')

    if skip is None: skip = []
    api = lib.getApi()

    if users is None:
        logger.info('Loading users...')
        users = read_users()
        logger.info(f'Done! There are {len(users)} users. {len(users[~(users.followed_at.isna())])} followed')

    if 'follow' in skip:
        pass
    else:
        # follow nr_new_following users
        logger.info('Following new users...')

        clf = pickle.load(open(f'{artefacts_path}/users_model.pkl', 'rb'))
        candidate_followers = users[users.followed_at.isna() & users.relation.isna()]
        candidate_followers = candidate_followers.drop(['scraped_from', 'username', 'biography', 'followed_at', 'unfollowed_at', 'relation'], axis=1).fillna(0)
        candidate_followers.loc[:, 'pred_relation'] = clf.predict(candidate_followers)

        logger.info(f'Nr of candidates: {len(candidate_followers[candidate_followers.pred_relation.isin([1, 2])])}')
        max_idx = -1

        try:
            for idx, row in tqdm(candidate_followers[candidate_followers.pred_relation.isin([1, 2])][:nr_new_following].iterrows(), total=nr_new_following):
                api.follow(row.pk)
                result = api.LastJson
                if api.LastResponse.status_code == 200:
                    if result['friendship_status']['following'] or result['friendship_status']['outgoing_request']:
                        users.loc[idx, 'followed_at'] = datetime.datetime.now()
                        users.loc[idx, 'followed_at'] = pd.to_datetime(users.loc[idx, 'followed_at'])
                    else:
                        logger.info(f"Following: {result['friendship_status']['following']}, "
                                    f"Outgoing request: {result['friendship_status']['outgoing_request']}")
                    sleep(1)
                elif api.LastResponse.status_code == 429:
                    sleep(300)
                max_idx = idx
        except KeyboardInterrupt:
            logger.info('Interrupted by user. Saving file, then exiting.')
            write_users(users)
            exit()

        logger.info(f'Unfollowed {max_idx} people.')

    if 'unfollow' in skip:
        pass
    else:
        logger.info('Unfollowing old followers...')
        # Select accounts followed before 2 days ago that haven't been unfollowed yet
        old_followers = users[(users.followed_at < datetime.datetime.now() - datetime.timedelta(days=2))
                              & (users.unfollowed_at.isna())]
        for idx, row in tqdm(old_followers.iterrows(), total=len(old_followers)):
            api.unfollow(row.pk)
            if api.LastResponse.status_code == 200:
                users.loc[idx, 'unfollowed_at'] = datetime.datetime.now()
                sleep(1)
            elif api.LastResponse.status_code == 429:
                sleep(300)
        logger.info(f'Unfollowed {len(old_followers)} people')

    write_users(users)
    end_time = datetime.datetime.now()
    logger.info(f'End time: {end_time}')
    logger.info(f'Duration: {end_time - start_time}')


def check_responses_of_follows():
    logger.info(f'{"#"*10} Check responses of follows {"#"*10}')
    start_time = datetime.datetime.now()
    logger.info(f'Start time: {start_time}')
    api = lib.getApi()

    logger.info('Loading users...')
    users = read_users()
    logger.info(f'Done! There are {len(users)} users. {len(users[~(users.followed_at.isna())])} followed')

    # Get followers and commenters and likers of recent images
    logger.info('Checking recently followed users\' responses')
    api.getSelfUserFeed()
    l_and_c = []
    recent_images = api.LastJson['items'][:20]
    for image in tqdm(recent_images):
        api.getMediaComments(f'{image["pk"]}')
        comments = api.LastJson['comments']
        l_and_c += [comment['user']['pk'] for comment in comments]

        api.getMediaLikers(image['pk'])
        likers = api.LastJson['users']
        l_and_c += [user['pk'] for user in likers]
    api.getSelfUserFollowers()
    followers = [user['pk'] for user in api.LastJson['users']]

    # update status accordingly
    users_needing_updates = users[users.relation.isna()
                                 & (users.followed_at < datetime.datetime.now() - datetime.timedelta(days=2))
                                 & (~users.followed_at.isna())]
    for idx, row in tqdm(users_needing_updates.iterrows(), total=len(users_needing_updates)):
        if row.pk in l_and_c and row.pk in l_and_c:
            users.loc[idx, 'relation'] = 2
            continue
        elif row.pk in followers:
            users.loc[idx, 'relation'] = 1
            continue
        else:
            users.loc[idx, 'relation'] = 0
            continue
    logger.info(f'Updated {len(users_needing_updates)} users.')

    write_users(users)
    end_time = datetime.datetime.now()
    logger.info(f'End time: {end_time}')
    logger.info(f'Duration: {end_time - start_time}')


def like_random_posts():
    logger.info(f'{"#"*10} like_random_posts {"#"*10}')

    api = lib.getApi()

    api.getTimeline()
    recent_posts = api.LastJson['items']
    logger.info('Liking posts...')
    for post in tqdm(recent_posts):
        if random.random() > 0.5:
            try:
                api.like(post['pk'])
            except:
                pass
            sleep(1)


def scrape_new_users():
    logger.info(f'{"#"*10} scrape new users {"#"*10}')

    api = lib.getApi()

    users = read_users()
    accounts = [x for x in open(f'{artefacts_path}/ig_users.txt').read().splitlines() if 'amsterdam' in x]

    user_ids = []
    user_accounts = []
    for x in accounts:
        api.searchUsername(x)
        if api.LastResponse.status_code != 200:
                print(f'Response was {api.LastResponse.status_code} {api.LastResponse.text} for user {x}')
                continue
        user_id = api.LastJson['user']['pk']
        user_accounts.append(x)
        user_ids.append(user_id)

    tracking_accounts = pd.DataFrame({'username': user_accounts, 'user_id': user_ids})

    # Get Likers and commenters
    new_users = []

    for idx, account in tqdm(tracking_accounts.iterrows(), total=len(tracking_accounts)):
        api.getUserFeed(account.user_id)

        userfeed = api.LastJson['items']
        image_pks = []
        for image in tqdm(userfeed):
            image_pks.append(image['pk'])
            api.getMediaLikers(image_pks[-1])
            liked_users = api.LastJson['users']
            for user in liked_users:
                user_info = {}
                user_info['pk'] = user['pk']
                user_info['username'] = user['username']
                user_info['is_private'] = user['is_private']
                user_info['is_verified'] = user['is_verified']
                user_info['scraped_from'] = account.username
                new_users.append(user_info)

            api.getMediaComments(f'{image_pks[-1]}')
            comments = api.LastJson['comments']
            for comment in comments:
                user = comment['user']
                user_info = {}
                user_info['pk'] = user['pk']
                user_info['username'] = user['username']
                user_info['is_private'] = user['is_private']
                user_info['is_verified'] = user['is_verified']
                user_info['scraped_from'] = account.username
                new_users.append(user_info)
    users = users.append(new_users)

    # get followers
    followers = []

    for idx, account in tqdm(tracking_accounts.iterrows(), total=len(tracking_accounts)):
        api.getUserFollowers(account.user_id)
        scraped_followers = api.LastJson['users']
        for user in tqdm(scraped_followers):
            user_info = {}
            user_info['pk'] = user['pk']
            user_info['username'] = user['username']
            user_info['is_private'] = user['is_private']
            user_info['is_verified'] = user['is_verified']
            user_info['scraped_from'] = account.username
            followers.append(user_info)
    users = users.append(followers)
    users = users.drop_duplicates(subset=['pk'], keep='first')
    users = users.reset_index(drop=True)
    write_users(users)


def retrieve_additional_information(users=None, start=None, nr_rows=None):
    logger.info(f'{"#"*10} retrieve_additional_information {"#"*10}')
    api = lib.getApi()

    if users is None:
        users = read_users()
    nr_rows = nr_rows if nr_rows else len(users)
    start = start if start else 0
    df = users.copy()

    # get rows where additional info is missing.
    if 'fo_fo_ratio' in df.columns:
        df = df[df['fo_fo_ratio'].isna()]
    df = df.iloc[start: start+min(nr_rows, 2000)]

    new_data = pd.DataFrame()
    logger.info(f'Scraping additional information...')
    for idx, user in tqdm(df.iterrows(), total=len(df)):
        api.searchUsername(user.username)
        logger.debug(f'api.searchusername response: {api.LastResponse.status_code}')
        if api.LastResponse.status_code == 200:
            try:
                j = api.LastJson['user']
                user = {}
                user['idx'] = idx
                user['fo_fo_ratio'] = j['follower_count'] / j['following_count'] if j['following_count'] != 0 else 0
                user['has_pf'] = not j['has_anonymous_profile_picture']
                user['biography'] = j['biography']
                user['is_business'] = j['is_business']
                user['media_count'] = j['media_count']
                user['total_igtv_videos'] = j['total_igtv_videos']
                new_data = new_data.append(user, ignore_index=True)

            except KeyError:
                logger.info(f'User not found: {user.username}')
                pass
            except KeyboardInterrupt:
                logger.info('Interrupted by user. Saving file, then exiting.')
#                 new_data.to_csv(f'{artefacts_path}/additional_information_users_temp.csv', sep=';', index=False)
                new_data = new_data.set_index('idx')
                users.update(new_data)
                write_users(users)
                exit()
            except Exception as e:
                logger.info(f'{e} for user {user.username}')
            sleep(2)
        elif api.LastResponse.status_code == 404:
            logger.info(f'User {user.username} not found')
            users = users.drop(idx)
        elif api.LastResponse.status_code == 429:
            logger.info('Waiting 5 minutes due to request limit')
            sleep(300)
        else:
            Exception(f'Unrecognized response code: {api.LastResponse.status_code}')
    new_data = new_data.set_index('idx')
    users.update(new_data)
    write_users(users)


def update_followers_command():
    update_followers()


def check_responses_of_follows_command():
    check_responses_of_follows()


def scrape_new_users_command():
    scrape_new_users()


def retrieve_additional_information_command():
    retrieve_additional_information()


if __name__ == "__main__":
    run()
